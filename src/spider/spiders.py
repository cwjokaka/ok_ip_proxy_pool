from typing import List, Iterable
from src.entity.proxy_entity import ProxyEntity
from src.enum.common import ProxyCoverEnum, ProxyTypeEnum
from src.log.logger import logger
from src.spider.abs_spider import AbsSpider
from bs4 import BeautifulSoup, Tag


spider_collection = {}


def spider_register(cls):
    spider_collection.update({cls.__name__: cls()})
    logger.info(f'æ³¨å†Œ{cls.__name__}')
    return cls


@spider_register
class Spider66Ip(AbsSpider):
    """
    66IPä»£ç†çˆ¬è™« åˆ·æ–°é€Ÿåº¦:ğŸŒæ…¢
    http://www.66ip.cn/
    """
    def __init__(self) -> None:
        super().__init__('66IPä»£ç†çˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tr_list = soup.find('table', attrs={'width': '100%', 'bordercolor': '#6699ff'}).find_all('tr')
        for i, tr in enumerate(tr_list):
            if i == 0:
                continue
            contents = tr.contents
            ip = contents[0].text
            port = contents[1].text
            region = contents[2].text
            proxy_cover = contents[3].text
            result.append(ProxyEntity(f'http://{ip}:{port}',
                                      source=self._name,
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return ['http://www.66ip.cn']

    def get_page_range(self) -> Iterable:
        return range(1, 6)

    def get_page_url(self, url, page) -> str:
        return f'{url}/{page}.html'

    def get_encoding(self) -> str:
        return 'gb2312'

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'é«˜åŒ¿ä»£ç†':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderQuanWangIp(AbsSpider):
    """
    å…¨ç½‘IPä»£ç†çˆ¬è™« åˆ·æ–°é€Ÿåº¦:æå¿«
    http://www.goubanjia.com/
    """
    def __init__(self) -> None:
        super().__init__('å…¨ç½‘IPä»£ç†çˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tr_list = soup.find('tbody').find_all('tr')
        for i, tr in enumerate(tr_list):
            tds = tr.find_all('td')
            id_and_port = tds[0]
            ip, port = self._parse_ip_and_port(id_and_port)
            proxy_cover = tds[1].text
            proxy_type = tds[2].text
            region = tds[3].contents[1].text
            supplier = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      supplier=supplier,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region
                                      )
                          )
        return result

    def get_urls(self) -> List[str]:
        return ['http://www.goubanjia.com']

    def get_page_url(self, url, page) -> str:
        return url

    def _parse_ip_and_port(self, ip_td: Tag):
        res = []
        contents = ip_td.find_all(['div', 'span'])
        for content in contents:
            res.append(content.text)
        res.pop()
        ip = ''.join(res)

        port_tag = contents[-1]
        port_ori_str = port_tag.get('class')[1]
        # è§£ç çœŸå®çš„ç«¯å£
        port = 0
        for c in port_ori_str:
            port *= 10
            port += (ord(c) - ord('A'))
        port /= 8
        port = int(port)
        return ip, str(port)

    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'é€æ˜':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'é«˜åŒ¿':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderXiciIp(AbsSpider):
    """
    è¥¿åˆºä»£ç†çˆ¬è™« åˆ·æ–°é€Ÿåº¦:ğŸŒæ…¢
    åŸºæœ¬ä¸Šæ²¡å‡ ä¸ªä»£ç†ä¸ªèƒ½ç”¨ğŸ†’
    https://www.xicidaili.com/
    """
    def __init__(self) -> None:
        super().__init__('è¥¿åˆºIPä»£ç†çˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        tab = soup.find('table', attrs={'id': 'ip_list'})
        if tab is None:
            return []
        tr_list = tab.find_all('tr')[1: -1]
        for tr in tr_list:
            tds = tr.find_all('td')
            ip = tds[1].text
            port = tds[2].text
            proxy_cover = tds[4].text
            proxy_type = tds[5].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      ))
        return result

    def get_urls(self) -> List[str]:
        return [
            'https://www.xicidaili.com/nn',     # é«˜åŒ¿
            'https://www.xicidaili.com/nt'      # é€æ˜
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 3)

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'é«˜åŒ¿':
            return ProxyCoverEnum.HIGH_COVER.value
        if cover_str == 'é€æ˜':
            return ProxyCoverEnum.TRANSPARENT.value
        else:
            return ProxyCoverEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_type(type_str: str):
        if type_str == 'HTTPS':
            return ProxyTypeEnum.HTTPS.value
        if type_str == 'HTTP':
            return ProxyTypeEnum.HTTP.value
        else:
            return ProxyTypeEnum.UNKNOWN.value


@spider_register
class SpiderKuaiDaiLiIp(AbsSpider):
    """
    å¿«ä»£ç†IP åˆ·æ–°é€Ÿåº¦: æå¿«
    https://www.kuaidaili.com/free
    """
    def __init__(self) -> None:
        super().__init__('å¿«ä»£ç†IPä»£ç†çˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        trs = soup.find('table').find('tbody').find_all('tr')
        for tr in trs:
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      # ip, port, protocol=proxy_type.lower(),
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return [
            'https://www.kuaidaili.com/free/inha',     # é«˜åŒ¿
            'https://www.kuaidaili.com/free/intr'      # é€æ˜
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 3)

    # çˆ¬å¤ªå¿«ä¼šè¢«å°
    def get_interval(self) -> int:
        return 3

    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'é€æ˜':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'é«˜åŒ¿å':
            return ProxyCoverEnum.HIGH_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderYunDaiLiIp(AbsSpider):
    """
    äº‘ä»£ç†IP åˆ·æ–°é€Ÿåº¦: å¿«
    http://www.ip3366.net/free
    """
    def __init__(self) -> None:
        super().__init__('äº‘ä»£ç†IPçˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        trs = soup.find('table').find('tbody').find_all('tr')
        for tr in trs:
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return [
            'http://www.ip3366.net/free/?stype=1',     # é«˜åŒ¿
            'http://www.ip3366.net/free/?stype=2'      # é€æ˜ or æ™®åŒ¿
        ]

    def get_page_range(self) -> Iterable:
        return range(1, 3)

    def get_page_url(self, url, page) -> str:
        return f'{url}&page={page}'


    def _judge_proxy_type(self, type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    def _judge_proxy_cover(self, cover_str: str):
        if cover_str == 'é€æ˜ä»£ç†IP':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'é«˜åŒ¿ä»£ç†IP':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'æ™®é€šä»£ç†IP':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderIpHaiIp(AbsSpider):
    """
    IPæµ·ä»£ç†IP åˆ·æ–°é€Ÿåº¦: 8åˆ†é’Ÿ/1ä¸ª
    æœ‰æ—¶ä¼šè¿ä¸ä¸Š
    http://www.iphai.com
    """
    def __init__(self) -> None:
        super().__init__('IPæµ·ä»£ç†IPçˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        table = soup.find('table')
        if table is None:
            return []
        tbody = soup.find('tbody')
        if tbody is None:
            return []
        trs = tbody.find_all('tr')
        for i, tr in enumerate(trs):
            if i == 0:
                continue
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text if tds[3].text != '' else 'http'
            region = tds[4].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_urls(self) -> List[str]:
        return [
            'http://www.iphai.com/free/ng',         # å›½å†…é«˜åŒ¿
            'http://www.iphai.com/free/np',         # å›½å†…æ™®é€š
            'http://www.iphai.com/free/wg',         # å›½å¤–é«˜åŒ¿
            'http://www.iphai.com/free/wp',         # å›½å¤–æ™®é€š
        ]

    # çˆ¬å¤ªå¿«ä¼šè¢«å°
    def get_interval(self) -> int:
        return 2

    def get_page_url(self, url, page) -> str:
        return url

    @staticmethod
    def _judge_proxy_type(type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'é€æ˜':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'é«˜åŒ¿':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'æ™®åŒ¿':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value


@spider_register
class SpiderMianFeiDaiLiIp(AbsSpider):
    """
    å…è´¹ä»£ç†IPåº“
    http://ip.jiangxianli.com/
    """
    def __init__(self) -> None:
        super().__init__('å…è´¹ä»£ç†IPçˆ¬è™«')

    def do_crawl(self, resp) -> List[ProxyEntity]:
        result = []
        soup = BeautifulSoup(resp, 'lxml')
        table = soup.find('table')
        if table is None:
            return []
        tbody = soup.find('tbody')
        if tbody is None:
            return []
        trs = tbody.find_all('tr')
        for i, tr in enumerate(trs):
            if i == 0:
                continue
            tds = tr.find_all('td')
            ip = tds[0].text
            port = tds[1].text
            proxy_cover = tds[2].text
            proxy_type = tds[3].text if tds[3].text != '' else 'http'
            region = tds[5].text
            supplier = tds[6].text
            result.append(ProxyEntity(f'{proxy_type.lower()}://{ip}:{port}',
                                      source=self._name,
                                      supplier=supplier,
                                      proxy_type=self._judge_proxy_type(proxy_type),
                                      proxy_cover=self._judge_proxy_cover(proxy_cover),
                                      region=region))
        return result

    def get_interval(self) -> int:
        return 2

    def get_page_range(self) -> Iterable:
        return range(1, 4)

    def get_urls(self) -> List[str]:
        return ['http://ip.jiangxianli.com/?page={}']

    def get_page_url(self, url, page) -> str:
        return url.format(page)

    @staticmethod
    def _judge_proxy_type(type_str: str):
        type_low = type_str.lower()
        if type_low == 'http':
            return ProxyTypeEnum.HTTP.value
        elif type_low == 'https':
            return ProxyTypeEnum.HTTPS.value
        else:
            return ProxyTypeEnum.UNKNOWN.value

    @staticmethod
    def _judge_proxy_cover(cover_str: str):
        if cover_str == 'é€æ˜':
            return ProxyCoverEnum.TRANSPARENT.value
        elif cover_str == 'é«˜åŒ¿':
            return ProxyCoverEnum.HIGH_COVER.value
        elif cover_str == 'æ™®åŒ¿':
            return ProxyCoverEnum.NORMAL_COVER.value
        else:
            return ProxyCoverEnum.UNKNOWN.value
